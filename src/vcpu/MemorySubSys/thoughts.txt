Ok, I will try to consolidate some thoughts here instead of scattered across the source..


* L1 Cache should work on virtual addresses, one per core
* MMU should be _BELOW_ Cache, one MMU per core
* DataBus below MMU, shared among all cores

NOTE: This is only with respect to Data Cache. An Instr. Cache is more less just hooked up differently...

so cpu does:
    memory.read()
        cache kick's in (is this in L1) -> yes, all good, return
            talk to MMU to fetch the data
                MMU de-translates the address
                    Issues a statement to the DataBus to fetch

Need to consider a few things here - who talks to the MMU?
Will the DataBus work with already translated addresses?
Or should the DataBus talk to the MMU - but the DataBus is a singleton - thus - it needs
to know the MMU (which is a problem). The other way is letting the MMU proxy the DataBus and thus intercept the calls
and translate on the fly...   => need to isolate this design and test it out..





Tech details about how I store virtual addresses:
        addresses are always 64bit, a mapped pages is looked up like:

        [8][8][8][....][12] =>
         8 bit table-index
         8 bit descriptor-index
         8 bit page-index
         [12 bits unused]        <- Here we drop in the 'VAddrStart' as 0x8000 0000
         12 bit page-offset

        static const uint64_t VIRTUAL_ADDR_SPACE_START = 0x80000000;

    // The MMU computes a virtual address according to this formula:
    uint64_t virtualAddress = (virtualRoot | virtualDesc | viurtualPage) | VIRTUAL_ADDR_SPACE_START;

    This means I am keeping the lower 4gb of memory reserved for the system.

    A page is 4k in size.
    There are (per process) 3 levels of page-table data, gives: 256*256*256*4096 => 67 Tb of addressable RAM
    "which should be enough for everyone..."


When refactoring take a look at RISC-V, https://github.com/riscv-software-src/riscv-isa-sim
And: https://ics.uci.edu/~swjun/courses/2021W-CS152/slides/lec10%20-%20Virtual%20memory.pdf
Specifically slides 8.. and then 18..
Pay care to slide 15 and 16 (TLB)
Also decide if cache comes before TLB or after (see slides 25..30)

Move this to 'MemorySubSys' and integrate with CPUMemCache

Slide 39 provides a nice overview of HW/SW

When allocating a page we need to make sure that 'VirtualRAM' doesn't start at address 0
Like we can reserve the first 4gb of virtual memory to something else...

Eviction policy - I need to figure this out. Currently we have a counter-based system simulating a 'time' like system.

MMU:
====
MMU is per Core.
Must support mapping of different virtual regions to different physical addresses..
Virtual => Physical
0....X  => physical region ABC
X....Y  => physical region DEF
Y....Z  => physical region GHI
.. und so weiter

Each region has it's own data-bus and translation rules (?) => Allows Flash/SPI/Fast/OnChip/etc mapped RAM
A region is defined by a lower range + size => effectively gives a bit-mask..

Each region also has a control-configuration, residing in the region lower X bytes.

PageTables are fully controlled by the OS. Even physical. The MMU only has the ability to traverse the page-table
for virtual to physical lookup and raise exceptions depending on flags (or special address, like -1).

In order to emulate this the PageAllocator EMU must be talk to the CPUBase class for any memory access (this allows
PageTables to be read from emulated RAM through the MMU and in to the L1 cache).
This allows page-table synchronization to happen automatically as they are loaded on a per-core basis!

Since Page-Tables are unique per process. But the physical memory table is not. We must switch to 'supervisor' mode (disabling
the mmu) everytime we have to update the physical page bitmap table.

Development wise;
1) Implement the regions (as of now; reserve 8 control registers in the regions upper part)
2) Implement supervisor mode (i.e. disabling of address translation) => pass through
3) Implement a PageAllocation handler which uses the CPU memory functions to handle the page tables

How does 'pass through' work with the L1 cache (should it work with the L1 cache? => YES!)?
    - Physical Addr => cacheline descriptor?
    - Need at least a flag to signal this cacheline was physical (MESI states are just 4 bits - we have 4 bits to spare!)

CacheController must call MMU to translate before calling data-bus read/write...
Make the cache-controller part of the MMU???

The page-table address is in a separate space(?).



L1 Cache:
=========
A cache line is 64bytes. Number of lines 256 => 16kb L1 Cache.
Note: Most modern CPU's today have 32kb or 64kb L1 cache, but cache-line sizes are more or less all 64bytes.
Some have longer L2 caches (Apple Silicon, 128bytes).

Cache-Line is currently direct-mapped (i.e linear look-up across all cache-lines).
We should try limit the search-space. For instance, split/hash the first look up (using some parts from the V-Addr)
then search the array. => n-set associative cache;
set = cachelineSets[hash(vAddr)];    // return search set; this is just an index * n (in n-set); like: index * 8
idxCacheLine = find_set_index(set, vAddr);   // find vAddr in set or not..

A set is just another way to 'split' the cache-line array - so no need to reorganize the data...

Important here is to make the emulation API agnostic to this and allow for different implementations in order to
test performance across various algorithms for lookup.

Consider data-structure in L1 Cache!

L2 Cache:
=========
Currently not simulated/emulated. We will hit the memory bus directly in case of an L1 Cache miss.
I should include a L2 Cache which is more or less attached to the DataBus. In essence the data-bus can first check
the L2 cache before fetching from RAM.


Questions for myself:
=====================
The MMU is 'below' (architecturally - being used by) the L1 Cache. But I would assume when the MMU read's the
translation table data into memory it will land in the L1 Cache. Otherwise the MMU would need to implement MESI
which would make it somewhat complicated - and we want to keep it simple.

Same goes for tracking of physical memory pages (the bitmap) in the MMU. This bitmap _MUST_ be shared between the cores.
Thus, it must be L1/L2 Cache aware. Or is this read/written all the time - explicitly through the MESI Data Bus?

OR - do the MMU have it's own L1 cache?

Some papers suggest that the MMU has it's own (private?) cache. Is this perhaps a separate L1 instance attached to the MMU?